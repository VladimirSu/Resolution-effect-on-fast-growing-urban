{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc7736a9-152c-4bbd-9393-c23b3df23a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-14 00:41:30.368191: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import sklearn.metrics \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "# Implement Learning rate decay\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint,LearningRateScheduler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv3D\n",
    "from tensorflow.keras.layers import ConvLSTM2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from math import sqrt  \n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import requests\n",
    "import os\n",
    "import h5py\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f314b98-179c-4cfb-b582-8868866c66b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_from_raw():\n",
    "# 遍历当前子目录内所有图像，逐个处理\n",
    "    dataset = []\n",
    "    idx = 0\n",
    "    for i in tqdm(range(1, 10)):\n",
    "        for j in range(1, 11):\n",
    "            data = []\n",
    "            for year in range(2009, 2018):\n",
    "                file = \"target_200/\" + str(year) +\"_\"+ str(i) +\"_\"+ str(j) + \".jpg\"\n",
    "                # 读入hdf5格式的图像数据，并转换为numpy数组        \n",
    "                img = Image.open(file)\n",
    "                original_image = np.array(img)\n",
    "                original_image = original_image.reshape(1,original_image.shape[0], original_image.shape[1], original_image.shape[2])\n",
    "                #归一化\n",
    "                original_image = original_image / 255.0\n",
    "                \n",
    "                #将所有年份的数据放入data\n",
    "                if(year == 2009):\n",
    "                    data = original_image\n",
    "                else:\n",
    "                    data = np.concatenate((data,original_image),axis=0)\n",
    "            \n",
    "            #将数据reshape为（sample， year， height， width， depth）\n",
    "            data = data.reshape(1, data.shape[0], data.shape[1], data.shape[2], data.shape[3])\n",
    "            if(idx == 0):\n",
    "                dataset = data\n",
    "            else:\n",
    "                dataset = np.concatenate((dataset,data),axis=0)\n",
    "            idx += 1\n",
    "    # 将当前批次内所有图像的处理结果存入`dataset`数组中\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df68ce99-d393-4578-987e-e36bf5c3836d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_xy(data, timesteps):\n",
    "    x = []\n",
    "    y = []\n",
    "    a = 2 * timesteps\n",
    "    \n",
    "    for i in tqdm(range(0, data.shape[0])):\n",
    "        for j in range(0, 10 - a):\n",
    "            temp = data[i, j:j+a,:,:,:]\n",
    "            x_temp = temp[:timesteps, :, :, :]\n",
    "            y_temp = temp[timesteps: , :, :, :]\n",
    "            x_temp = x_temp.reshape(1, x_temp.shape[0], x_temp.shape[1], x_temp.shape[2], x_temp.shape[3])\n",
    "            y_temp = y_temp.reshape(1, y_temp.shape[0], y_temp.shape[1], y_temp.shape[2], y_temp.shape[3])\n",
    "\n",
    "            if(i == 0):\n",
    "                x = x_temp\n",
    "                y = y_temp\n",
    "            else:\n",
    "                x = np.concatenate((x,x_temp),axis=0)\n",
    "                y = np.concatenate((y,y_temp),axis=0)\n",
    "            \n",
    "            del(x_temp)\n",
    "            del(y_temp)\n",
    "            del(temp)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856178eb-12b0-4ab4-94d7-db64746675a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6/9 [00:18<00:12,  4.03s/it]"
     ]
    }
   ],
   "source": [
    "dataset = create_dataset_from_raw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80200a0e-1d0b-4a75-abda-c399256aa189",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9791e6d-2c36-4e6d-90ad-d0dcac745d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 2\n",
    "dataset_x, dataset_y = split_data_xy(dataset, timesteps)\n",
    "train_X, test_X, train_y, test_y = train_test_split(dataset_x,dataset_y,test_size=0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2dfcf6-7032-4bd3-a8b9-6a8971ba321b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ee58c0-91d9-4698-bf91-e834ff4bc791",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(ConvLSTM2D(filters=64, kernel_size=(7, 7),\n",
    "             input_shape=(timesteps, 200, 200, 3),\n",
    "             padding='same', activation=LeakyReLU(alpha=0.01), return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(ConvLSTM2D(filters=64, kernel_size=(5, 5),\n",
    "                    padding='same', activation=LeakyReLU(alpha=0.01), return_sequences=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(ConvLSTM2D(filters=64, kernel_size=(3, 3),\n",
    "                    padding='same', activation=LeakyReLU(alpha=0.01), return_sequences=True))\n",
    "model.add(Conv3D(filters=3, kernel_size=(3, 3, 3),\n",
    "         activation='sigmoid',\n",
    "         padding='same', data_format='channels_last'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "keras.utils.plot_model(model, to_file=\"model.png\")\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c519bec-3fc6-4f39-b890-35931b088d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"200_timesteps2.h5\",      # 修改\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          min_delta = 0, \n",
    "                          patience = 5,\n",
    "                          verbose = 2,\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss',\n",
    "                              factor = 0.2,\n",
    "                              patience = 3,\n",
    "                              verbose = 1)\n",
    "                              #min_delta = 0.00001)\n",
    "\n",
    "callbacks = [earlystop, checkpoint, reduce_lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cdbc18-37c3-4a11-a15d-94476afc8e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练模型\n",
    "history = model.fit(train_X, train_y, epochs=100, batch_size=32, validation_data=(test_X, test_y), verbose=2, shuffle=False, callbacks = callbacks)\n",
    "\n",
    "# 对损失进行可视化\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test') \n",
    "plt.title(\"Model Loss\\n\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40cd5aa-4572-43fa-95db-202361c19f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = np.array(history.history['loss'])\n",
    "val_loss = np.array(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a29abbc-7fdb-43b9-8883-055bc8e3646a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = loss.reshape(loss.shape[0], 1)\n",
    "val_loss = val_loss.reshape(val_loss.shape[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af141283-d21e-4f8b-a208-157273fb7400",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_all = np.hstack((loss, val_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217f0475-c2a1-48d6-bf80-cbc1009a20c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_all = pd.DataFrame(loss_all)\n",
    "loss_all.to_csv(r'loss.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f9e8fb-c981-4dff-bfd2-bbc20e7a148a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
